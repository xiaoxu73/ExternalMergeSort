# 海量数据排序系统

该系统用于对海量数据进行外部排序，特别适用于内存受限环境下的大数据处理。

## 项目结构

```
.
├── bin/                 # 编译后的可执行文件目录
│   └── merge_sort_tests # 测试可执行文件
├── include/             # 头文件目录
│   ├── external_merge_sort.h  # 外部排序类声明
│   └── thread_pool.h          # 线程池类声明
├── src/                 # 源代码目录
│   ├── external_merge_sort.cpp  # 外部排序类实现
│   ├── generate_data.cpp        # 测试数据生成器实现
│   └── thread_pool.cpp          # 线程池类实现
├── test/                # 测试代码目录
│   └── merge_sort_test.cpp      # Google Test测试用例
├── CMakeLists.txt       # CMake构建配置文件
└── README.md            # 项目说明文档
```

## 功能特点

1. **线程池支持**: 利用多核CPU提高排序效率
2. **外部排序**: 处理远超内存容量的数据
3. **内存限制友好**: 在小内存环境下也能正常工作
4. **负载均衡**: 合理分配任务给多个工作线程
5. **无锁设计**: 最大限度减少线程同步开销

## 技术方案

### 核心算法
使用外部归并排序算法：
1. **分割与预排序阶段**: 将大文件分割成内存可容纳的小块，分别排序并保存为临时文件
2. **多路归并阶段**: 将所有临时文件进行多路归并，生成最终有序文件

### 线程池设计
- 使用生产者-消费者模式
- 任务队列采用互斥锁保护
- 支持动态提交异步任务

### 锁优化策略
- 粗粒度锁：任务队列操作使用互斥锁
- 免锁/无锁：数据处理阶段各线程独立工作，无需同步

## 可执行文件说明

目前项目提供以下可执行文件：

1. **merge_sort_tests**: 包含完整的测试套件，用于验证系统的正确性和性能

## 编译说明

### 方法一：使用 Make
```bash
make
```

### 方法二：使用 CMake
```bash
mkdir build
cd build
cmake ..
make
```

## 测试说明

项目使用 Google Test 进行单元测试和性能测试，测试内容包括：

1. **基本功能测试**: 验证小数据集的正确性
2. **大数据集测试**: 验证大数据集的处理能力
3. **混合文件大小测试**: 处理不同大小文件的能力
4. **低内存限制测试**: 在极低内存限制下的稳定性
5. **大量文件测试**: 处理大量小文件的能力
6. **边界条件测试**: 空文件、单文件等特殊情况
7. **真实随机数据测试**: 使用generate_test_data函数生成的真实随机分布数据集

### 运行测试

使用 Make 运行测试：
```bash
make test
```

或者直接运行测试可执行文件：
```bash
./bin/merge_sort_tests
```

每个测试用例都会输出详细的信息，包括：
- 数据集大小和文件数量
- 排序耗时
- 内存占用情况

## 设计细节

### 内存管理
- 严格遵守用户设定的内存限制
- 动态计算每个阶段可使用的数据量
- 合理划分缓冲区避免内存溢出

### 负载均衡
- 文件处理任务自动分配给空闲线程
- 大文件会自动分块处理
- 避免某些线程过载而其他线程空闲

### 性能优化
- 减少不必要的数据复制
- 使用二进制文件格式提高IO效率
- 采用高效的STL排序算法

## 系统要求

- Linux操作系统
- 支持C++17的编译器（如GCC 7+）
- 足够的磁盘空间（至少是原始数据大小的1.5倍）
- Google Test（用于运行测试）

## 参数说明

### 内存限制
默认使用64MB内存，可通过命令行参数调整。实际使用中会根据该值动态计算每次处理的数据量。

### 文件格式
所有数据文件均为二进制格式，每个数据占8字节（64位有符号整数）。